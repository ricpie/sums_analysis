---
title: "sumsarized"
author: "berkeley_air_monitoring_group"
date: "Apr-3 2018"
output:
  html_document:
    toc: true
    toc_float: true
    theme: united
    self_contained: yes
---

```{r read_me}
  #To run this code for SUMs analysis, first ensure that Rstudio with R 3.3.2 or later is installed.
  #The code requires 1. SUMSARIZED data, and 2. a tracking data Excel sheet
  #Ensure that the files are located in the right locations, as per the SUMs Analysis Training Guide (https://www.dropbox.com/s/wcykl2wsb67tuux/SUMSAnalysisBATraining.pdf?dl=0)
```

# Define variables for coding stoves and for filtering out data.
* Also set some constants for the stove event analysis

```{r define_variables}
  #Users should must make sure the two variables in this chunk are defined as desired.

  #1. Set to TRUE if we need to reimport data SUMSARIZED, but this is slower.  Set to FALSE if already imported and no new data has been added.
  reimport_data <- FALSE
  
  #2.  Make sure the parameter file is for the project you are using.  The parameter file must be located in the r_files folder.
  source('../r_files/parameter_file_Nigeria_AfDB.R') 
```

```{r load_libraries_functions}
  source('../r_scripts/load.R')
  source("../r_scripts/functions.R")
  source("../r_scripts/load_data.r")
  source("../r_scripts/plots.R")
  source("../r_scripts/filter_sum_data.R")
  if (.Platform$OS.type!="unix") {
        Sys.setenv("R_ZIPCMD" = "C:/Rtools/bin/zip.exe") 
    }
```


```{r global_options, include=FALSE}
  knitr::opts_chunk$set(fig.path='../figures/', warning=FALSE, message=FALSE, cache=FALSE)
```

# Import temperature data and metadata files
```{r load_data, warning=FALSE}
  #Loads all SUMSARIZED data.  Reloads the RDS file if reimport_data is set to FALSE.  This is faster if all the data has been previously imported by setting it to TRUE.
  
  if (!file.exists("../r_files/sumsarized.RDS") | reimport_data == TRUE) {
  #Rprof("rprofout.out") #Analyze code, it is quite slow... but only need to do it occasionally.
  saveRDS(load_sumsarized(stove_codes$stove), file ="../r_files/sumsarized.RDS")
  #Rprof(NULL)
  #summaryRprof("../r_markdown/rprofout.out")
  }

  #Save the sumsarized data to a single dataframe called sumsarized.RDS, located in r_files.
  sumsarized <- readRDS("../r_files/sumsarized.RDS") 

  #Load metadata from the tracking sheet.  
  saveRDS(load_meta_download(path_tracking_sheet), file = "../r_files/metadata_download.RDS")
  metadata <- readRDS("../r_files/metadata_download.RDS")
  metadata <- dplyr::filter(metadata,!(is.na(filename)))

  print(HHID_remove) #List of houses that have been flagged for exclusion
  print(bad_files) #List of files that have been flagged for exclusion
```

# Tidy

* add household id information, get metadata from metadata file and from file names, depending on country
```{r parse_add_metadata}
#Import thermocouple data time series output from Sumsarizer.
#datetime_placed is currently based on the metadata.  Flags data as good/bad based on the bad_files and HHID_remove variables.  Those are reapplied directly to the cooking events later, and filtered out if bad.
  if ( reimport_data == TRUE) {
    
      sumsarized_filtered <- filter_sumsarized(sumsarized,metadata,bad_files,HHID_remove,
                                               project_name,stove_codes,logging_duration_minimum) #HHID is grabbed from the filename.
      saveRDS(sumsarized_filtered, file ="../r_files/sumsarized_filtered.RDS")
  } else {
      sumsarized_filtered <- readRDS("../r_files/sumsarized_filtered.RDS") #Loads the already-imported filtered sumsarized data if reimport_data is set to FALSE.
  }

#These are the stove types found in the file names
print(unique(sumsarized_filtered$stove))

#This is a list of the stove types listed in the parameter file.  Any stove types missing from here that are in the file names will appear as 0 or NA in the analysis later, and must be 
#corrected by adding it to the stove_codes list.
kable(stove_codes) 
```


# Form cooking events from raw sumsarizer output 
* Group distinct cooking events together.  If they are within cooking_group minutes of each other.
* Need this for data that went through the web-based sumsarizer, where events are not made automatically.

```{r group_cooking_events, warning=FALSE}

# # start counting up for cooking events.  Add columns for start and stop times of cooking events, and keep the hhid, loggerid, stove type, field group.  Keep only cooking event time points.  Need a case for start (if i = 1), if data point is within 30 minutes of the last one, if data point is greater than 30 minutes from the last one.

# #Initialize the cooking_events frame.
 cooking_events <- data.frame(start_time=as.POSIXct(character()),
                    end_time=as.POSIXct(character()), group=factor(),  HHID=factor(),
                    logger_id=factor(),stove=factor(), logging_duration_days=as.numeric(),
                    datetime_placed = as.POSIXct(character()), datetime_removal = as.POSIXct(character()),
                    file_indices = as.numeric(), filename=character(),fullsumsarizer_filename=character(),comments=character(),
                    qc_dates=character(),use_flag = as.logical())

 #for each SUM data file
 for (i in unique(sumsarized_filtered$file_indices)) {
   #Grab data from file i, and keep only the entries that are marked as cooking
   temp <- dplyr::filter(sumsarized_filtered,file_indices == i) %>%
      dplyr::filter(state == TRUE) %>% dplyr::filter(!duplicated(datetime)) 
    tempALL <- dplyr::filter(sumsarized_filtered,file_indices == i) 
   
   qc_temp <- "ok" #Default to ok data quality, gets demoted based on later checks.

   if (dim(temp)[1]>1 && any(temp$state==TRUE)) {
      min_time_file = min(tempALL$datetime)
      max_time_file = max(tempALL$datetime)
      time_difference <- as.numeric(difftime(temp$datetime,lag(temp$datetime),units="mins"))
      time_difference <- time_difference[!is.na(time_difference)] #
     
      breakstart <- c(0,which((time_difference>cooking_group) == TRUE))+1 #Start of a cooking event
      breakend <- c(which((time_difference>cooking_group) == TRUE),
          if (tail(temp$state,n=1) == TRUE){dim(temp)[1]}) #End of cooking event. 
        #Tail part is in case the time series ends while still cooking...need to account for th

      #Organize/check/fix datetimes of sum placements and removals. If there is a start and end time available for the monitoring period, use it to keep the data from the file.  Disregard this if the datetime_removal is NA, since it means we don't have a fixed known end date.  In this case we assume the end of the file is the end of the monitoring preiod and keep all the data from the given file.  Provide the start and end date in the event-building loop below.
       if (max_time_file>end_date_range | min_time_file<start_date_range) {   #datetime_placed is changed to the local file value if it is below the start_date_range, and datetime_placed is too, according to the end_date_range
             datetime_placed_temp = min_time_file
             datetime_removal_temp = max_time_file 
             qc_temp <- "out_of_placement_range" #These get filtered out, assuming it represents poorly time formatted data.
       } else if (is.na(as.POSIXct(temp$datetime_removal[1])) | is.na(as.POSIXct(temp$datetime_placed[1]))){  #If date is NA, use the file's dates.  They have already been midnight-trimmed in the load_data.r function.
             datetime_placed_temp = min_time_file
             datetime_removal_temp = max_time_file
             qc_temp <- "NA_metadata"
       } else if (min_time_file> as.POSIXct(temp$datetime_placed[1])) {  #If placement time is before time of first data point, switch to first data point.
             datetime_placed_temp = min_time_file
             datetime_removal_temp = max_time_file
             qc_temp <- "placement_before_data"
       } else if (as.POSIXct(min(temp$datetime_placed))> as.POSIXct(max(temp$datetime_removal))) {  #If placement time is greater than the datetime placed, switch them, it's a mistake.
             datetime_placed_temp = min_time_file
             datetime_removal_temp = max_time_file 
             qc_temp <- "placement_greaterthan_removal"
       } else { #If not NA, a value must have been found in the meta data, use it.
             datetime_placed_temp = as.POSIXct(min(temp$datetime_placed))
             datetime_removal_temp = as.POSIXct(max(temp$datetime_removal))
             qc_temp <- "metadata_dates_used"
       }      
      
      #Add cooking events to the cooking_events data frame.
      cooking_events <- rbind(cooking_events,
                  data.frame(start_time= as.POSIXct(temp$datetime[breakstart]),
                  end_time=as.POSIXct(temp$datetime[breakend]), 
                  group=as.factor(temp$group[breakstart]),
                  HHID=as.factor(temp$HHID[breakstart]),
                  logger_id=factor(temp$logger_id[breakstart]),
                  stove=factor(temp$stove[breakstart]),
                  logging_duration_days = as.numeric(difftime(datetime_removal_temp,datetime_placed_temp,units = "days")),
                  datetime_placed = datetime_placed_temp,
                  datetime_removal = datetime_removal_temp,
                  file_indices = temp$file_indices[breakstart], 
                  filename = temp$filename[breakstart],
                  fullsumsarizer_filename = temp$fullsumsarizer_filename[breakstart],
                  comments = temp$comments[1],
                  use_flag=as.logical(rep(1,length(breakstart)[1])),
                  qc_dates = qc_temp))
    } else{
         #If no cooking events are found, still create an entry, though with the use flag as FALSE, so 
         #that we know that there was data collected and zero events in that period.  Set the use_flag to true here to differntiate from the too-short cooking events.
       temp <- dplyr::filter(sumsarized_filtered,file_indices == i)  #Create new temp here so 
       #we can get info about the sample that does not have cooking events.
       cooking_events <- rbind(cooking_events,
                  data.frame(start_time=as.POSIXct(temp$datetime[1]),
                  end_time=as.POSIXct(temp$datetime[1]), 
                  group=as.factor(temp$group[1]),
                  HHID=as.factor(temp$HHID[1]), 
                  logger_id=factor(temp$logger_id[1]),
                  stove=factor(temp$stove[1]), 
                  logging_duration_days = as.numeric(difftime(max(temp$datetime),min(temp$datetime),units = "days")),
                  datetime_placed = min(temp$datetime),
                  datetime_removal = max(temp$datetime),
                  file_indices = temp$file_indices[1], 
                  filename = temp$filename[1],
                  fullsumsarizer_filename = temp$fullsumsarizer_filename[1],
                  comments = temp$comments[1],
                  qc_dates = qc_temp,
                  use_flag=FALSE))
    }
   
 }


#Clean up cooking events. Implements quality checking in the qc variable so we know which events to remove later.  Also adds better variable names for stoves, and units.
 cooking_events <- dplyr::left_join(cooking_events,
                                              dplyr::select(stove_codes,stove,stove_descriptions),by = "stove") %>%
                          dplyr::mutate(units = "degrees Celsius") %>% #Define units as degrees C
                          dplyr::mutate(duration_minutes = as.numeric(difftime(end_time,start_time,units = "mins"))) %>%
                          dplyr::mutate(qc = if_else(grepl(bad_files,fullsumsarizer_filename,ignore.case=TRUE),
                                                     "excluded_file_list","ok")) %>% #Flag data from bad files
                          dplyr::mutate(qc = if_else(grepl(HHID_remove,HHID),
                                                     "excluded_household_list",qc)) %>% #Flag data from HHID's that should be removed.
                          dplyr::mutate(qc = if_else(grepl("amb",fullsumsarizer_filename,ignore.case=TRUE),"ambient",qc))  #Flag data from HHID's that should be removed.
               
```


# Remove data from bad files:
  * merge flags with data
  *Perform filtering on cooking events variable to get to the subset we want.
```{r create_flags}

#Removes events that are classified as too short, while keeping entries from files with no events.
#Adds new time-related variables for later plotting and analysis.
#Drops unneeded variables.
ok_cooking_events_unfiltered <-  dplyr::mutate(cooking_events,qc,if_else(is.na(stove_descriptions),"NA_stove_description",qc)) %>%
        #Filter out events shorter than the threshold, while keeping the entries that 
        #have no uses in the deployments. At this point all events except empty files have use_flag = TRUE.
        dplyr::mutate(if_else(grepl("out_of_placement_range",qc_dates),"out_of_placement_range",qc)) %>%
        dplyr::select(filename,fullsumsarizer_filename,start_time,end_time,duration_minutes,
              HHID,stove,logger_id,group,
              stove_descriptions,stove,logging_duration_days,datetime_placed,
              datetime_removal, units,comments,use_flag,qc) %>%
        dplyr::mutate(start_hour = hour(start_time) + minute(start_time)/60) %>%
        dplyr::mutate(month_year = format(start_time,"%b-%y")) %>%
        dplyr::mutate(month_year = factor(month_year, unique(month_year), ordered=TRUE)) %>%
        dplyr::mutate(day_month_year = as.Date(start_time)) %>%
        dplyr::mutate(week_year = format(start_time,"%V-%y")) %>%
        dplyr::mutate(day_of_week = as.factor(weekdays(start_time))) %>%
        dplyr::mutate(day_of_week = factor(day_of_week, 
                   levels = c('Monday','Tuesday','Wednesday','Thursday',
                              'Friday','Saturday','Sunday'))) %>%
        dplyr::arrange(desc(datetime_removal)) %>%  #Sort so we toss the earlier end dates when deleting distinct values.
        dplyr::distinct(start_time,duration_minutes,HHID,stove_descriptions, .keep_all = TRUE) %>% 
        #Handle cases that have duplicated events due to overlapping files. Keep only distinct ones
        dplyr::mutate(datetime_placed = floor_date(datetime_placed,"minute")) %>%
        dplyr::mutate(qc=if_else(logging_duration_days < logging_duration_minimum,"short_file_duration",qc)) %>%#Each file must be longer than this many days
        dplyr::arrange(HHID) %>%
        dplyr::mutate(qc=if_else(is.na(HHID),"NA_HHID",qc)) %>%
        dplyr::mutate(qc=if_else(is.na(datetime_placed),"NA_datetime_placed",qc)) 
  
#Removes data with qc listed as anything but 'ok'. 
ok_cooking_events <- dplyr::filter(ok_cooking_events_unfiltered,grepl("ok",qc)) %>% #Keep only data marked 'ok'
        droplevels() #Getting rid of extra levels whos data was filtered out, so unique doesn't freak out.

#Get list of files removed, and reasons why:
not_ok_cooking_events <- dplyr::filter(ok_cooking_events_unfiltered,!grepl("ok",qc)) %>% #Keep only data marked NOT 'ok'
        droplevels() #Getting rid of extra levels whos data was filtered out, so unique doesn't freak out.

list_of_bad_files <-  dplyr::mutate(not_ok_cooking_events,logging_duration_days = as.character(sprintf("%0.2f", round(logging_duration_days, digits = 2)  ))) %>%
        dplyr::select(fullsumsarizer_filename,start_time,qc,HHID,logging_duration_days,datetime_placed,stove_descriptions) %>%
        dplyr::distinct(fullsumsarizer_filename,qc, .keep_all = TRUE)
kable(list_of_bad_files) #Plot the list of bad files so we can look at them and re-analyze after correcting any mistakes.


#Create padded time series of cooking events, so there is an event for every day, rather than gaps.  This facilitates 
ok_cooking_events_padded <- data.frame(stringsAsFactors = FALSE)
#Replace NA with 0.
#dplyr_if_else   <- function(x) { mutate_all(x, funs(ifelse(is.na(.), 0, .))) }
uniquers <- unique(ok_cooking_events$fullsumsarizer_filename)
for (i in 1:length(unique(ok_cooking_events$filename))) {

        temp <- dplyr::filter(ok_cooking_events,uniquers[i]==fullsumsarizer_filename) %>%
                dplyr::arrange(start_time)
        
    if (dim(temp)[1]>0) {    
        # generate a time sequence with 1 day intervals to fill in
        # missing dates
        all.dates <- data.frame(dates = seq(temp$datetime_placed[1], temp$datetime_removal[1], by="day"),stringsAsFactors = FALSE) %>%
                  dplyr::mutate(day_month_year = as.Date(dates)) %>%
                  dplyr::filter(!day_month_year %in% temp$day_month_year) #Get rid of extra days
        
        # Convert all dates to a data frame. Note that we're putting
        # the new dates into a column called "start_time" just like the
        # original column. This will allow us to merge the data.
        all.dates.frame <- data.frame(list(start_time=all.dates$dates),list(end_time=all.dates$dates), 
                                      list(day_month_year = as.Date(all.dates$dates)),
                                      list(week_year = format(all.dates$dates,"%V-%y")),
                                      list(day_of_week = weekdays(all.dates$dates)),stringsAsFactors = FALSE) %>%
                  dplyr::mutate(month_year = format(start_time,"%b-%y")) %>%
                  dplyr::mutate(month_year = factor(month_year, unique(month_year), ordered=TRUE))
                         

        # Merge the two datasets: the full dates and original data
        merged.data <- merge(all.dates.frame, temp, all=T) %>%
            tidyr::fill(filename,fullsumsarizer_filename,HHID,stove,logger_id,group,
                        stove_descriptions,logging_duration_days,datetime_placed,
                        datetime_removal,units,comments,
                        start_hour,.direction = c("up")) %>%
            tidyr::fill(filename,fullsumsarizer_filename,HHID,stove,logger_id,group,
                        stove_descriptions,logging_duration_days,datetime_placed,
                        datetime_removal,units,comments,
                        start_hour,.direction = c("down"))  %>%
            dplyr::mutate(use_flag = replace(use_flag, is.na(use_flag), FALSE)) 
        
        
        merged.data %>% mutate_if(is.factor, as.character) -> merged.data

        merged.data$use_flag[is.na(merged.data$use_flag)] <- FALSE
        merged.data[is.na(merged.data)] <- 0
        # The above merge set the new observations to NA.
        # To replace those with a 0, we must first find all the rows
        # and then assign 0 to them.
        #merged.data <-dplyr_if_else(merged.data)

        ok_cooking_events_padded <- rbind(ok_cooking_events_padded,merged.data)
    }
}
    #time format was messed up somehow in making the padded set, correct it here.
    ok_cooking_events_padded <- dplyr::mutate(ok_cooking_events_padded,datetime_removal = as.POSIXct(datetime_removal,origin = "1970-1-1", tz = "GMT")) %>%
      dplyr::arrange((start_time)) %>%
      dplyr::arrange(desc(stove_descriptions)) %>%  #Sort so we toss the earlier end dates when deleting distinct values.
      dplyr::mutate(month_year = format(start_time,"%b-%y")) %>%
      dplyr::mutate(month_year = factor(month_year, unique(month_year), ordered=TRUE)) %>%
      dplyr::mutate(qc = "ok") #New qc values default to zero during padding, but they are all already qc-filtered, so reset to TRUE

    #Finish filtering the dataset.  Kept 'bad' and short events to get the full padded dataset.  Now can remove them
    ok_cooking_events <- dplyr::filter(ok_cooking_events,duration_minutes>cooking_duration_minimum | use_flag==FALSE) 
    
    #Remove household-stoves that have less than this many days... do it to the padded data set or both?  Should be done somehow for the usage fraction results
    #dplyr::mutate(qc = if_else(logging_duration_days < total_days_logged_minimum,"short_total_logging_duration",qc)) %>% # Only keep data from house-stove combos with more than this number of days of data.
      
```



# Summarize data

```{r summarize_data}


ok_datasummary <- summarise_all(ok_cooking_events,funs(n_distinct(.)))


#Summarize use for each household
stats_grouped_by_hhid_only <- dplyr::group_by(ok_cooking_events_padded,HHID) %>%
    # calculate events per day for each household and stove.  Can then take summary stats of those.
    dplyr::summarise(events_per_day=sum(use_flag,na.rm=TRUE)/length(unique(day_month_year)),
                     minutes_per_day_stove_hhid = sum(duration_minutes,na.rm=TRUE)/length(unique(day_month_year)),
                     minutes_per_event_stove_hhid = sum(duration_minutes,na.rm=TRUE)/sum(use_flag,na.rm=TRUE),
                     days_logged_per_group_by_stove=sum(length(unique(day_month_year)), na.rm = TRUE),
                     events = n()) 
kable(stats_grouped_by_hhid_only, digits=2)

#Summarize use for each household and group
stats_grouped_by_hhid <- dplyr::group_by(ok_cooking_events_padded,HHID,group) %>%
    # calculate events per day for each household and stove.  Can then take summary stats of those.
    dplyr::summarise(events_per_day=sum(use_flag,na.rm=TRUE)/length(unique(day_month_year)),
                     minutes_per_day_stove_hhid = sum(duration_minutes,na.rm=TRUE)/length(unique(day_month_year)),
                     minutes_per_event_stove_hhid = sum(duration_minutes,na.rm=TRUE)/sum(use_flag,na.rm=TRUE),
                     days_logged_per_group_by_stove=sum(length(unique(day_month_year)), na.rm = TRUE),
                     events = n()) 
kable(stats_grouped_by_hhid, digits=2)

#Summarize use for each unique stove.  HHID by stove-descriptions, so if a household has two LPG stoves, and they have the same description, it will look like one stove.  If it has a description like LPG-left and LPG-right it will be seen as a different stove.
stats_grouped_by_stove <- dplyr::mutate(ok_cooking_events_padded,stove_hhid = paste0(HHID,stove_descriptions)) %>%
                    dplyr::group_by(stove_hhid) 
print(paste(length(unique(stats_grouped_by_stove$stove_hhid)), "stoves monitored in total"))


#Summarize use for each household and stove.
stats_grouped_by_hhid_stove <- dplyr::group_by(ok_cooking_events_padded,stove_descriptions,HHID,group) %>%
    # calculate events per day for each household and stove.  Can then take summary stats of those.
    dplyr::summarise(events_per_day=sum(use_flag,na.rm=TRUE)/length(unique(day_month_year)),
                     minutes_per_day_stove_hhid = sum(duration_minutes,na.rm=TRUE)/length(unique(day_month_year)),
                     minutes_per_event_stove_hhid = sum(duration_minutes,na.rm=TRUE)/sum(use_flag,na.rm=TRUE),
                     days_logged_per_group_by_stove=sum(length(unique(day_month_year)), na.rm = TRUE),
                     events = n()) 
kable(stats_grouped_by_hhid_stove, digits=2)


#Calculate summary for the overall groups and stoves from the household-wise results above.  Should this be weighted in case one hh has way more data?
stats_by_stove_group <- dplyr::group_by(stats_grouped_by_hhid_stove,stove_descriptions,group) %>%
    dplyr::summarise(mean_events_per_day=mean(events_per_day, na.rm = TRUE),
                       median_events_per_day=median(events_per_day, na.rm = TRUE),
                       sd_events_per_day=sd(events_per_day, na.rm = TRUE),
                       mean_minutes_per_day_stove_hhid=mean(minutes_per_day_stove_hhid, na.rm = TRUE),
                       median_minutes_per_day_stove_hhid=median(minutes_per_day_stove_hhid, na.rm = TRUE),
                       sd_minutes_per_day_stove_hhid=sd(minutes_per_day_stove_hhid, na.rm = TRUE),
                       mean_days_logged=mean(days_logged_per_group_by_stove, na.rm = TRUE),
                       house_stoves=n()) %>%
    dplyr::filter(!is.na(stove_descriptions)) 
kable(stats_by_stove_group, digits=2)


#Calculate summary for the overall stoves from the household-wise results above.  Should this be weighted in case one hh has way more data?
#Same as above but without the group, just by stove.
stats_by_stove <- dplyr::group_by(stats_grouped_by_hhid_stove,stove_descriptions) %>%
    dplyr::summarise(mean_events_per_day=mean(events_per_day, na.rm = TRUE),
                       median_events_per_day=median(events_per_day, na.rm = TRUE),
                       sd_events_per_day=sd(events_per_day, na.rm = TRUE),
                       mean_minutes_per_day_stove_hhid=mean(minutes_per_day_stove_hhid, na.rm = TRUE),
                       median_minutes_per_day_stove_hhid=median(minutes_per_day_stove_hhid, na.rm = TRUE),
                       sd_minutes_per_day_stove_hhid=sd(minutes_per_day_stove_hhid, na.rm = TRUE),
                        house_stoves=n()) %>%
    dplyr::filter(!is.na(stove_descriptions)) 
kable(stats_by_stove, digits=2)


#Summarize use for each household and stove, by month
stats_grouped_by_hhid_stove_month <- dplyr::group_by(ok_cooking_events_padded,stove_descriptions,HHID,group,month_year) %>%
    # calculate events per day for each household and stove.  Can then take summary stats of those.
    dplyr::summarise(events_per_day=sum(use_flag,na.rm=TRUE)/length(unique(day_month_year)),
                     minutes_per_day_stove_hhid = sum(duration_minutes,na.rm=TRUE)/length(unique(day_month_year)),
                     minutes_per_event_stove_hhid = sum(duration_minutes,na.rm=TRUE)/sum(use_flag,na.rm=TRUE),
                     days_logged_per_group_by_stove=sum(length(unique(day_month_year)), na.rm = TRUE))
kable(stats_grouped_by_hhid_stove_month, digits=2)


#Calculate summary for the overall groups and stoves from the household-wise results above. 
stats_by_stove_group_month <- dplyr::group_by(stats_grouped_by_hhid_stove_month,stove_descriptions,group,month_year) %>%
    dplyr::summarise(mean_events_per_day=mean(events_per_day, na.rm = TRUE),
                       median_events_per_day=median(events_per_day, na.rm = TRUE),
                       sd_events_per_day=sd(events_per_day, na.rm = TRUE),
                       mean_minutes_per_day_stove_hhid=mean(minutes_per_day_stove_hhid, na.rm = TRUE),
                       median_minutes_per_day_stove_hhid=median(minutes_per_day_stove_hhid, na.rm = TRUE),
                       sd_minutes_per_day_stove_hhid=sd(minutes_per_day_stove_hhid, na.rm = TRUE),
                       households=n(),
                       mean_days_logged=mean(days_logged_per_group_by_stove, na.rm = TRUE))
kable(stats_by_stove_group_month, digits=2)

#Write summary stats to an excel workbook
list_of_datasets <- list("By Stove Group and HHID" = stats_grouped_by_hhid_stove, "By Stove Group" = stats_by_stove_group,
                         "By Stove Type" = stats_by_stove,"By HHID and Monthly" = stats_grouped_by_hhid_stove_month,
                         "By Group and Monthly" = stats_by_stove_group_month)
openxlsx::write.xlsx(list_of_datasets, file = paste0("Summary Statistics ",format(now(),"%d-%b-%y"),".xlsx"))
```


# Plot all data time series
* Plot usage rates (uses/day) by stove type, and region
```{r plot_timeseries, fig.width=10, fig.height=70}

#Plot time series for each household, colored by filename.  For data completion purposes.
field_timeseries_plot(sumsarized_filtered, "stove_temp", "datetime", "HHID", "stove_descriptions","qc") 

```

# Plot data time series, removing flagged files and households
* Plot usage rates (uses/day) by stove type, and region
```{r plot_nonflagged_timeseries, fig.width=10, fig.height=70}
#Plot time series for each household, colored by filename.  Filtered out bad qc data
field_timeseries_plot(dplyr::filter(sumsarized_filtered,grepl("ok",qc)),"stove_temp", "datetime", "HHID", "stove_descriptions","qc") 

```

# List of files without associated HHID and metadata from the tracking sheet.
```{r list_orphaned_files, fig.width=10, fig.height=80}
files_without_hhid <- unique(dplyr::filter(sumsarized_filtered,is.na(HHID)) %>% dplyr::select(fullsumsarizer_filename))   #Data is removed in the next step if there is no HHID
files_without_hhid

files_without_metadata <- unique(dplyr::filter(sumsarized_filtered,is.na(stove_descriptions)) %>% dplyr::select(fullsumsarizer_filename))   #Note the name of files for which there is there is no matching filename in the tracking sheet, ergo no metadata from the tracking sheet. Not currently being filtered out, but this should be attended to by fixing any data files/tracking entries that appear in this variable.  Make sure that the actual file name matches what is listed in the tracking sheet.  Files names should also be unique.
files_without_metadata


filtered_files <- unique(dplyr::filter(sumsarized_filtered,grepl("ok",qc)) %>% dplyr::select(fullsumsarizer_filename))   #Note the name of files for which there is there is no matching filename in the tracking sheet, ergo no metadata from the tracking sheet. Not currently being filtered out, but this should be attended to by fixing any data files/tracking entries that appear in this variable.  Make sure that the actual file name matches what is listed in the tracking sheet.  Files names should also be unique.
```



# Boxplot usage results

```{r boxplots_usage, fig.width=7, fig.height=7}

give.n <- function(x){
   return(c(y = 0, label = length(x)))
}

give.mean <- function(x){
   return(c(y =mean(x)+.1, label = round(mean(x),digits=2)))
}


give.mean_minutes <- function(x){
   return(c(y =mean(x)+10, label = round(mean(x),digits=2)))
}

#To make box and whiskers quantiles rather than IQRs.
f <- function(x) {
  r <- quantile(x, probs = c(0.05, 0.25, 0.5, 0.75, 0.95))
  names(r) <- c("ymin", "lower", "middle", "upper", "ymax")
  r
}

#Plot average uses per day, by group, using the padded data set. (one data point per household-stove combo, separated by group)
grouped_data_means<- dplyr::group_by(ok_cooking_events_padded,stove_descriptions,HHID,group) %>%
  dplyr::summarise(avg_events_per_day=sum(use_flag,na.rm=TRUE)/length(unique(day_month_year))) 

g1 <- ggplot(grouped_data_means,aes(x=stove_descriptions, y = avg_events_per_day)) +
  stat_summary(fun.data = f, geom="boxplot") +  
  stat_summary(fun.y=mean, colour="blue", geom="point", 
               shape=18, size=4.5,alpha = 0.5)+
  stat_summary(fun.data = give.mean, geom = "text",colour="blue") + 
  geom_jitter(height = 0,width = 0.2,alpha = 0.25) +
  #facet_grid(stove_use_category~group,scales = "free", space = "free") + 
  stat_summary(fun.data = give.n, geom = "text") + 
  facet_grid(~group,scales = "free", space = "free") + 
  labs(y="Average uses per day",x="") + 
  ggtitle("Average uses per day for each household-stove combination") + 
  theme(axis.text.x = element_text(angle = 30, hjust = 1)) + 
  theme(legend.title = element_blank())
g1
#ggsave(filename="../figures/AverageUsesPerDay_byStoveGrouppadded.png", plot=g1,width = 6, height = 4)

#Plot average uses per day, NOT by group, using the padded data set. (one data point per household-stove combo)
stove_data_means<- dplyr::group_by(ok_cooking_events_padded,stove_descriptions,HHID) %>%
  dplyr::summarise(avg_events_per_day=sum(use_flag,na.rm=TRUE)/length(unique(day_month_year))) 

g11 <- ggplot(stove_data_means,aes(x=stove_descriptions, y = avg_events_per_day)) +
  stat_summary(fun.data = f, geom="boxplot") +  
  stat_summary(fun.y=mean, colour="blue", geom="point", 
               shape=18, size=4.5,alpha = 0.5)+
  stat_summary(fun.data = give.mean, geom = "text",colour="blue") + 
  geom_jitter(height = 0,width = 0.2,alpha = 0.25) +
  stat_summary(fun.data = give.n, geom = "text") + 
  labs(y="Average uses per day",x="") + 
  ggtitle("Average uses per day for each household-stove combination") + 
  theme(axis.text.x = element_text(angle = 30, hjust = 1)) + 
  theme(legend.title = element_blank())
g11
#ggsave(filename="../figures/AverageUsesPerDay_byStove.png", plot=g11,width = 6, height = 4)


#Plot uses per day, by group, using the padded data set. (one data point per day-household-stove combo)
g111<- dplyr::group_by(ok_cooking_events_padded, stove_descriptions, day_month_year, HHID, group) %>%
  summarise(events_per_day=sum(use_flag,na.rm=TRUE)) %>%
  ggplot(aes(events_per_day)) +
  geom_histogram() +
  #stat_summary(fun.data = f, geom="boxplot") +  
  #geom_jitter(height = 0,width = 0.2,alpha = 0.25) +
  #facet_grid(stove_use_category~group,scales = "free", space = "free") + 
  #stat_summary(fun.data = give.n, geom = "text") + 
  facet_grid(group~stove_descriptions,scales = "free", space = "free") + 
  labs(y="days",x="Number of uses by day") + 
  theme(axis.text.x = element_text(angle = 30, hjust = 1)) + 
  theme(legend.title = element_blank())
g111
##ggsave(filename="../figures/UsesPerDayDistribution_byGrouppadded.png", plot=g111,width = 6, height = 4)


#Plot average uses per day for all groups together
g2 <- dplyr::group_by(ok_cooking_events_padded,stove_descriptions,HHID) %>%
  dplyr::summarise(avg_events_per_day=sum(use_flag,na.rm=TRUE)/length(unique(day_month_year))) %>%
  ggplot(aes(x=stove_descriptions, y = avg_events_per_day)) +
  stat_summary(fun.data = f, geom="boxplot") +    
  stat_summary(fun.y=mean, colour="blue", geom="point", 
               shape=18, size=4.5,alpha = 0.5)+
  stat_summary(fun.data = give.mean, geom = "text",colour="blue") + 
  geom_jitter(height = 0,width = 0.2,alpha = 0.25) + 
  labs(y="Average uses/day",x="") + 
  stat_summary(fun.data = give.n, geom = "text") + 
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) 
g2
#ggsave(filename="../figures/AverageUsesPerDay_All.png", plot=g2,width = 6, height = 4)

#Plot average minutes used per day for each stove-household combination
g3<-  dplyr::group_by(ok_cooking_events_padded,stove_descriptions,HHID,group) %>%
  dplyr::summarise(avg_events_per_day=sum(duration_minutes,na.rm=TRUE)/length(unique(day_month_year))) %>%
  ggplot(aes(x=stove_descriptions, y = avg_events_per_day)) +
  stat_summary(fun.data = f, geom="boxplot") +   
  geom_jitter(height = 0,width = 0.2,alpha = 0.25) +
  stat_summary(fun.y=mean, colour="blue", geom="point", 
               shape=18, size=4.5,alpha = 0.5)+
  stat_summary(fun.data = give.mean_minutes, geom = "text",colour="blue") + 
  facet_grid(~group,scales = "free", space = "free") + 
  labs(y="Average minutes used per day",x="") + 
  stat_summary(fun.data = give.n, geom = "text") + 
  theme(axis.text.x = element_text(angle = 60, hjust = 1))
g3
#ggsave(filename="../figures/AverageTimeUsedPerDay_All.png", plot=g3,width = 6, height = 4)

#Average event duration
g4<-  dplyr::group_by(ok_cooking_events_padded,stove_descriptions,HHID,group) %>%
  dplyr::summarise(minutes_per_day_stove_hhid=sum(duration_minutes,na.rm=TRUE)/sum(use_flag,na.rm=TRUE)) %>%  
  ggplot(aes(x=stove_descriptions, y = minutes_per_day_stove_hhid)) + 
  stat_summary(fun.data = f, geom="boxplot") + 
  stat_summary(fun.y=mean, colour="blue", geom="point", 
               shape=18, size=4.5,alpha = 0.5)+
  stat_summary(fun.data = give.mean_minutes, geom = "text",colour="blue") + 
  geom_jitter(height = 0,width = 0.2,alpha = 0.25) +
  stat_summary(fun.data = give.n, geom = "text") + 
  labs(y="Average event duration by household (minutes)",x="") + 
  theme(axis.text.x = element_text(angle = 60, hjust = 1))
g4
#ggsave(filename="../figures/AverageTimeUsedPerEvent_byHousehold.png", plot=g4,width = 6, height = 4)


```

# Time trend plots

```{r plot_time_trends, fig.width=8, fig.height=7}


#Plot event duration per day by day of week.  Do not use the padded data set here, since there are non-events included in it, and they have an associated time.
g5 <- ggplot(ok_cooking_events, aes(x=day_of_week, y = duration_minutes)) + 
    geom_violin(fun.data = f) +    geom_jitter(height = 0,width = 0.2,alpha = 0.1) +
    facet_wrap(stove_descriptions~group,scales = "free") + 
    labs(y="Cooking event duration (minutes)",x="") + 
    theme(axis.text.x = element_text(angle = 60, hjust = 1))
g5
#ggsave(g5,filename=paste("../figures/DurationbyDayofWeek",y,".png",sep=""))

#Monthly usage summary

monthlyboxplots <-    ggplot(stats_grouped_by_hhid_stove_month,aes(x=month_year, y = events_per_day,color = stove_descriptions)) +
    stat_summary(fun.data = f, geom="boxplot",position = position_dodge(width=0.75)) +
    geom_jitter(alpha = 0.25,position = position_dodge(width=0.75)) +
    labs(y="Events/day",x="") + 
    facet_wrap(stove_descriptions~group,scales = "free") + 
    ggtitle("Monthly trends") + 
    theme(axis.text.x = element_text(angle = 60, hjust = 1),legend.title = element_blank())
monthlyboxplots
#ggsave(monthlyboxplots,filename=paste("../figures/monthlyboxplots",y,".png",sep=""),width = 7, height = 2)


#Plot %stove-days, where stove-day is defined as whether the stove is used on the given day or not.
  #Calculate percent stove-days.
g7 <-dplyr::group_by(ok_cooking_events_padded,day_month_year,stove_descriptions,group) %>%
      dplyr::distinct(HHID,stove_descriptions,day_month_year, .keep_all = TRUE) %>% #No duplicate events from the same hh contributing to stove-days.
      dplyr::mutate(days_used_day_stove_grouped = 100*sum(use_flag, na.rm=TRUE)/(sum(use_flag, na.rm=TRUE)+sum(!use_flag, na.rm=TRUE))) %>%
      ggplot(aes(x=day_month_year, y = days_used_day_stove_grouped,color = stove_descriptions)) + 
    geom_point(alpha = 0.1) + geom_smooth() + #method='lm',formula=y~x) +
    facet_grid(~group,scales = "free", space = "free") + 
    labs(y="% stove days",x="") + 
    theme(legend.title = element_blank()) + 
    theme(axis.text.x = element_text(angle = 60, hjust = 1))
  g7
          #ggsave(g7,filename=paste("../figures/PercentStoveDaysByGroup.png",sep=""))


#Time of day trend.  Based on start time, change it to middle of event?  Remove groups with less than 5 hh's?
#Use unpadded set to get time of day of event starts.
g8 <- ggplot(ok_cooking_events, aes(start_hour, fill = stove_descriptions, 
                      colour = stove_descriptions)) + geom_density(alpha = 0.1) +
      labs(y="Time of day use density",x="Hour of day") +
      ggtitle("stove use density") + 
      facet_grid(~group,scales = "free", space = "free") + 
      theme(plot.title = element_text(hjust = 0.5)) +
      scale_x_continuous(breaks=seq(0,24,6)) + 
      ylim(0, 0.15)
g8
#ggsave(g8,filename=paste("../figures/Density",y,".png",sep=""))


#Usage time series by minutes used each day, by group and stove type  
 g12<- dplyr::group_by(ok_cooking_events_padded,stove_descriptions,day_month_year,group) %>%
  dplyr::summarise(mean_daily_use_tseries=mean(duration_minutes,na.rm=TRUE)) %>%
  ggplot(aes(day_month_year, y=mean_daily_use_tseries,color = stove_descriptions)) + #,label=sprintf("%0.2f", round(usefraction, digits = 2)))) +
  geom_smooth()+
  geom_point(alpha = 0.25)+
  facet_grid(~group~stove_descriptions,scales = "free", space = "free") + 
  ggtitle("Daily average uses by group and stove") + 
  labs(y="Minutes per day",x="") +
  ylim(0,200)
 g12
#ggsave(filename="../figures/Overall_usage_fraction.png", plot=g12,width = 6, height = 4)

```

# Usage fraction result plots

```{r plot_usage_fractions, fig.width=7, fig.height=7}

#Daily usage fraction by group. Includes all hh.  We could filter out houses with fewer than x sums to reduce any potential bias from a certain sum type burning up or something.
g10<- dplyr::select(ok_cooking_events_padded,stove_descriptions,start_time,duration_minutes) %>%
  thicken('day', col = 'day') %>% 
  group_by(stove_descriptions, day) %>%
  summarise(avg=mean(duration_minutes,na.rm=TRUE)) %>%
  ggplot(aes(day, avg)) +
  geom_bar(aes(fill = stove_descriptions), 
           col = "black",
           stat = 'identity', 
           position = "fill",size=0) +
  ggtitle("Usage fraction") + 
  labs(y="Usage fraction",x="")  +
  theme(legend.title = element_blank())
g10


#Overall usage fraction by group.  
 g11<- dplyr::select(ok_cooking_events_padded,stove_descriptions,day_month_year,duration_minutes) %>%
  group_by(stove_descriptions) %>%
  summarise(avg=mean(duration_minutes,na.rm=TRUE)) %>%
  dplyr::mutate(usefraction = avg/sum(avg)) %>%
  ggplot(aes(x=1, usefraction,label=sprintf("%0.2f", round(usefraction, digits = 2)))) +
  geom_col(aes(fill=stove_descriptions))+
  geom_text(aes(group=stove_descriptions),position = position_stack(vjust = 0.5)) +
  ggtitle("Overall usage fraction") + 
  labs(y="Usage fraction",x="")  +
  theme(legend.title = element_blank())
g11
#ggsave(filename="../figures/Overall_usage_fraction.png", plot=g12,width = 6, height = 4)


#Overall usage fraction by group.  
 g111<- dplyr::select(ok_cooking_events_padded,stove_descriptions,day_month_year,duration_minutes,group) %>%
  group_by(stove_descriptions,group) %>%
  summarise(avg=mean(duration_minutes,na.rm=TRUE,keep_all=TRUE)) %>%
  group_by(group) %>%
  dplyr::mutate(usefraction = avg/sum(avg)) %>%
  dplyr::filter(usefraction>0.001) %>%
  ggplot(aes(x=1, usefraction*.8,label=sprintf("%0.2f", round(usefraction, digits = 2)))) +
  geom_col(aes(fill=stove_descriptions))+
  geom_text(aes(group=stove_descriptions),position = position_stack(vjust = 0.5)) +
  ggtitle("Overall usage fraction") + 
  facet_grid(~group,scales = "free", space = "free") + 
  labs(y="Usage fraction",x="")  +
  theme(legend.title = element_blank())
g111
#ggsave(filename="../figures/Overall_usage_fraction_bygroup.png", plot=g111,width = 6, height = 4)


```

# Plot deployment details 

```{r plot_monitoring_diagnostics, fig.width=7, fig.height=7}

 #Barplot by day of fraction of stoves monitored... not too interesting but neat coding to look at.
g9<- dplyr::select(ok_cooking_events_padded,stove_descriptions,start_time,duration_minutes) %>%
  thicken('day', col = 'day') %>% #Map data to a higher level variable.  Compresses two data points from one day into one day
  count(stove_descriptions, day) %>%
  ggplot(aes(day, n)) +
  ggtitle("Daily stove monitored by fraction") + 
  labs(y="Monitored fraction",x="")  +
  geom_bar(aes(fill = stove_descriptions), 
           col = "black",
           stat = 'identity', 
           position = "fill", size=0)
g9

#Plot days sampled per file
g6<-  dplyr::distinct(ok_cooking_events_padded,fullsumsarizer_filename,logging_duration_days,stove_descriptions) %>% 
  ggplot(aes(x=stove_descriptions, y = logging_duration_days)) + 
  stat_summary(fun.data = f, geom="boxplot") + geom_jitter(width = 0.05,alpha = 0.25) + 
  #geom_text_repel( aes(label= ifelse(logging_duration_days > 30,as.character(fullsumsarizer_filename),''))) + 
  #facet_grid(~group,scales = "free", space = "free") + 
  labs(y="Days logged per file",x="") + 
  theme(axis.text.x = element_text(angle = 60, hjust = 1))
g6
#ggsave(filename="../figures/DaysSampled_byGroup.pdf", plot=g6)
```



# Plot stacking 

```{r plot_stacking, fig.width=6, fig.height=4}
stacking_stovedays <- stacking_stovedays_fun(ok_cooking_events_padded) #Use the stacking_stovedays_fun function from functions.r
give.mean5 <- function(x){
   return(c(y =mean(x)+5, label = round(mean(x),digits=1)))
}
gg_stack <- ggplot(stacking_stovedays,aes(x=stove_descriptions, y = percent_stovedays,color = stove_descriptions)) +
    stat_summary(fun.data = f, geom="boxplot") +
    geom_jitter(alpha = 0.25,position = position_dodge(width=0.75)) +
    labs(y="Percent stove-days",x="") + 
    facet_wrap(~group,scales = "free") + 
    stat_summary(fun.y=mean, colour="blue", geom="point", 
               shape=18, size=4,alpha = 0.5)+
    stat_summary(fun.data = give.mean5, geom = "text",colour="blue",alpha = 0.5,size = 3) + 
    stat_summary(fun.data = give.n, geom = "text",colour = "black",alpha = 0.5,size = 3) + 
  
    theme(axis.text.x = element_text(angle = 60, hjust = 1),legend.title = element_blank())
gg_stack
ggsave(filename="../figures/stacking_plot.png", plot=gg_stack,width = 8, height = 4)


```

# Summary

Temperature was measured for `r length(unique(ok_cooking_events$HHID))` houses between `r min(ok_cooking_events$start_time, na.rm = TRUE)` and `r max(ok_cooking_events$end_time, na.rm = TRUE)`. There are no cooking events for homes: `r setdiff(as.character(metadata$HHID), as.character(ok_cooking_events$HHID))`.

Temperature data is expected to be missing for: no tests.

# Save files

* save data

```{r save_data}
  saveRDS(ok_cooking_events, file = "../r_files/ok_cooking_events.RDS")
  saveRDS(ok_cooking_events_padded, file = "../r_files/ok_cooking_events_padded.RDS")
```
