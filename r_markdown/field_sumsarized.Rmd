---
title: "sumsarized"
author: "ricardo_piedrahita"
date: "Apr-3 2018"
output:
  html_document:
    toc: true
    toc_float: true
    theme: united
    self_contained: yes
---

# Define variables for coding stoves and for filtering out data.
* Also set some constants for the stove event analysis

```{r define_variables}
  reimport_data <- FALSE #Set to TRUE if we need to reimport data SUMSARIZED, but this is slower.  Set to FALSE if already imported and no new data has been added.

  source('../r_files/parameter_file_Nigeria_AfDB.R')
  Removed_Houses <- print(HHID_remove)
  Files_with_these_strings_removed <- print(bad_files)
  print(stove_codes)
```

```{r global_options, include=FALSE}
  knitr::opts_chunk$set(fig.path='../figures/', warning=FALSE, message=FALSE, cache=FALSE)
```

```{r functions_libs}
  source('../r_scripts/load.R')
  source("../r_scripts/functions.R")
  source("../r_scripts/load_data.r")
  source("../r_scripts/plots.R")
  source("../r_scripts/filter_sum_data.R")
```

# Import temperature data and metadata files
```{r load_data, warning=FALSE, echo=FALSE}
  #Reloads all SUMSARIZED data.
  
  if (!file.exists("../r_files/sumsarized.RDS") | reimport_data == TRUE) {
  #Rprof("rprofout.out") #Analyze code, it is quite slow... but only need to do it occasionally.
  saveRDS(load_sumsarized(stove_codes$stove,project_name), file ="../r_files/sumsarized.RDS")
  #Rprof(NULL)
  #summaryRprof("../r_markdown/rprofout.out")
  }

  sumsarized <- readRDS("../r_files/sumsarized.RDS") #

  #Load metadata
  saveRDS(load_meta_download(path_tracking_sheet), file = "../r_files/metadata_download.RDS")
  metadata <- readRDS("../r_files/metadata_download.RDS")
```

# Tidy

* add household id information, get metadata from metadata file and from file names, depending on country
```{r parse_add_metadata}
#Import thermocouple data time series output from Sumsarizer.
#datetime_placed is currently based on the metadata.  Flags data as good/bad based on the bad_files and HHID_remove variables.  Those are reapplied directly to the cooking events later, and filtered out if bad.
  if ( reimport_data == TRUE) {
      sumsarized_filtered <- filter_sumsarized(sumsarized,metadata,bad_files,HHID_remove,project_name,stove_codes) #HHID is grabbed from the filename.
          saveRDS(sumsarized_filtered, file ="../r_files/sumsarized_filtered.RDS")
  } else {
      sumsarized_filtered <- readRDS("../r_files/sumsarized_filtered.RDS") #
  }
```

  
* Form cooking events from raw sumsarizer output 
* Group distinct cooking events together.  If they are within cooking_group minutes of each other.
* Only need this for data that went through the web-based sumsarizer, where events are not made automatically.

```{r group_cooking_events, warning=FALSE}

# # start counting up for cooking events.  Add columns for start and stop times of cooking events, and keep the hhid, loggerid, stove type, field group.  Keep only cooking event time points.  Need a case for start (if i = 1), if data point is within 30 minutes of the last one, if data point is greater than 30 minutes from the last one.

# #Initialize the cooking_events frame.
 cooking_events <- data.frame(start_time=as.POSIXct(character()),
                    end_time=as.POSIXct(character()), group=factor(),  HHID=factor(),
                    logger_id=factor(),stove=factor(), logging_duration_days=as.numeric(),
                    datetime_placed = as.POSIXct(character()), datetime_removal = as.POSIXct(character()),
                    file_indices = as.numeric(), filename=character(),fullsumsarizer_filename=character(),comments=character(),
                    qc_dates=character(),use_flag = as.logical())

 #for each SUM data file
 for (i in unique(sumsarized_filtered$file_indices)) {
   #Grab data from file i, and keep only the entries that are marked as cooking
   temp <- dplyr::filter(sumsarized_filtered,file_indices == i) %>%
          dplyr::filter(state == TRUE) %>% dplyr::filter(!duplicated(datetime)) 
   qc_temp <- "ok" #Default to ok data quality, gets demoted based on later checks.

   if (dim(temp)[1]>1 && any(temp$state==TRUE)) {
      min_time_file = min(temp$datetime)
      max_time_file = max(temp$datetime)
      time_difference <- as.numeric(difftime(temp$datetime,lag(temp$datetime),units="mins"))
      time_difference <- time_difference[!is.na(time_difference)] #
     
      breakstart <- c(0,which((time_difference>cooking_group) == TRUE))+1 #Start of a cooking event
      breakend <- c(which((time_difference>cooking_group) == TRUE),
          if (tail(temp$state,n=1) == TRUE){dim(temp)[1]}) #End of cooking event. 
        #Tail part is in case the time series ends while still cooking...need to account for th

      #Organize/check/fix datetimes of sum placements and removals. If there is a start and end time available for the monitoring period, use it to keep the data from the file.  Disregard this if the datetime_removal is NA, since it means we don't have a fixed known end date.  In this case we assume the end of the file is the end of the monitoring preiod and keep all the data from the given file.  Provide the start and end date in the event-building loop below.
       if (max_time_file>end_date_range | min_time_file<start_date_range) {   #datetime_placed is changed to the local file value if it is below the start_date_range, and datetime_placed is too, according to the end_date_range
             datetime_placed_temp = min_time_file
             datetime_removal_temp = max_time_file 
             qc_temp <- "out_of_placement_range" #These get filtered out, assuming it represents poorly time formatted data.
       } else if (is.na(as.POSIXct(temp$datetime_removal[1])) | is.na(as.POSIXct(temp$datetime_placed[1]))){  #If date is NA, use the file's dates.  They have already been midnight-trimmed in the load_data.r function.
             datetime_placed_temp = min_time_file
             datetime_removal_temp = max_time_file
             qc_temp <- "NA_metadata"
       } else if (min_time_file> as.POSIXct(temp$datetime_placed[1])) {  #If placement time is before time of first data point, switch to first data point.
             datetime_placed_temp = min_time_file
             datetime_removal_temp = max_time_file
             qc_temp <- "placement_before_data"
       } else if (as.POSIXct(min(temp$datetime_placed))> as.POSIXct(max(temp$datetime_removal))) {  #If placement time is greater than the datetime placed, switch them, it's a mistake.
             datetime_placed_temp = min_time_file
             datetime_removal_temp = max_time_file 
             qc_temp <- "placement_greaterthan_removal"
       } else { #If not NA, a value must have been found in the meta data, use it.
             datetime_placed_temp = as.POSIXct(min(temp$datetime_placed))
             datetime_removal_temp = as.POSIXct(max(temp$datetime_removal))
             qc_temp <- "metadata_dates_used"
       }      
      
      #Add cooking events to the cooking_events data frame.
      cooking_events <- rbind(cooking_events,
                  data.frame(start_time= as.POSIXct(temp$datetime[breakstart]),
                  end_time=as.POSIXct(temp$datetime[breakend]), 
                  group=as.factor(temp$group[breakstart]),
                  HHID=as.factor(temp$HHID[breakstart]),
                  logger_id=factor(temp$logger_id[breakstart]),
                  stove=factor(temp$stove[breakstart]),
                  logging_duration_days = as.numeric(difftime(datetime_removal_temp,datetime_placed_temp,units = "days")),
                  datetime_placed = datetime_placed_temp,
                  datetime_removal = datetime_removal_temp,
                  file_indices = temp$file_indices[breakstart], 
                  filename = temp$filename[breakstart],
                  fullsumsarizer_filename = temp$fullsumsarizer_filename[breakstart],
                  comments = temp$comments[1],
                  use_flag=as.logical(rep(1,length(breakstart)[1])),
                  qc_dates = qc_temp))
    } else{
         #If no cooking events are found, still create an entry, though with the use flag as FALSE, so 
         #that we know that there was data collected and zero events in that period.  Set the use_flag to true here to differntiate from the too-short cooking events.
       temp <- dplyr::filter(sumsarized_filtered,file_indices == i)  #Create new temp here so 
       #we can get info about the sample that does not have cooking events.
       cooking_events <- rbind(cooking_events,
                  data.frame(start_time=as.POSIXct(temp$datetime[1]),
                  end_time=as.POSIXct(temp$datetime[1]), 
                  group=as.factor(temp$group[1]),
                  HHID=as.factor(temp$HHID[1]), 
                  logger_id=factor(temp$logger_id[1]),
                  stove=factor(temp$stove[1]), 
                  logging_duration_days = as.numeric(difftime(max(temp$datetime),min(temp$datetime),units = "days")),
                  datetime_placed = min(temp$datetime),
                  datetime_removal = max(temp$datetime),
                  file_indices = temp$file_indices[1], 
                  filename = temp$filename[1],
                  fullsumsarizer_filename = temp$fullsumsarizer_filename[1],
                  comments = temp$comments[1],
                  qc_dates = qc_temp,
                  use_flag=FALSE))
    }
   
 }


#Clean up cooking events. Add better variable names for stoves, and units.
 cooking_events <- dplyr::left_join(cooking_events,
                                              dplyr::select(stove_codes,stove,stove_descriptions),by = "stove") %>%
                          dplyr::mutate(units = "degrees Celsius") %>% #Define units as degrees C
                          dplyr::mutate(duration_minutes = as.numeric(difftime(end_time,start_time,units = "mins"))) %>%
                          dplyr::mutate(qc = if_else(grepl(bad_files,fullsumsarizer_filename,ignore.case=TRUE),"bad","ok")) %>% #Flag data from bad files
                          dplyr::mutate(qc = if_else(grepl(HHID_remove,HHID),"bad",qc)) %>% #Flag data from HHID's that should be removed.
                          dplyr::mutate(qc = if_else(grepl("amb",fullsumsarizer_filename,ignore.case=TRUE),"bad",qc))  #Flag data from HHID's that should be removed.
               
```


# Remove data from bad files:
  * merge flags with data
  *Perform filtering on cooking events variable to get to the subset we want.
```{r create_flags}

  #Remove short events, while keeping entries from files with no events.
  ok_cooking_events <-  dplyr::filter(cooking_events,!is.na(stove_descriptions)) %>%
                          dplyr::filter(logging_duration_days >= logging_duration_minimum) %>% # Only keep files longer than a day.
                          #Filter out events shorter than the threshold, while keeping the entries that 
                          #have no uses in the deployments. At this point all events except empty files have use_flag = TRUE.
                          dplyr::filter(duration_minutes>cooking_duration_minimum | use_flag==FALSE) %>%
                          dplyr::filter(!grepl("out_of_placement_range",qc_dates)) %>%#Keep data not marked as placed out of range
                          dplyr::select(filename,fullsumsarizer_filename,start_time,end_time,duration_minutes,
                                HHID,stove,logger_id,group,
                                stove_descriptions,stove,logging_duration_days,datetime_placed,
                                datetime_removal, units,comments,use_flag,qc) %>%
                          dplyr::mutate(start_hour = hour(start_time) + minute(start_time)/60) %>%
                          dplyr::mutate(month_year = format(start_time,"%b-%y")) %>%
                          dplyr::mutate(month_year = factor(month_year, unique(month_year), ordered=TRUE)) %>%
                          dplyr::mutate(day_month_year = as.Date(start_time)) %>%
                          dplyr::mutate(week_year = format(start_time,"%V-%y")) %>%
                          dplyr::mutate(day_of_week = as.factor(weekdays(start_time))) %>%
                          dplyr::mutate(day_of_week = factor(day_of_week, 
                                     levels = c('Monday','Tuesday','Wednesday','Thursday',
                                                'Friday','Saturday','Sunday'))) %>%
                          dplyr::arrange(desc(datetime_removal)) %>%  #Sort so we toss the earlier end dates when deleting distinct values.
                          dplyr::distinct(start_time,duration_minutes,HHID,stove_descriptions, .keep_all = TRUE) %>% 
                          #Handle cases that have duplicated events due to overlapping files. Keep only distinct ones
                          dplyr::mutate(datetime_placed = floor_date(datetime_placed,"minute")) %>%
                          dplyr::group_by(HHID,stove_descriptions) %>% # grouping to organize stats by deployment.
                          dplyr::mutate(days_logged_by_hhid_stove = length(unique(day_month_year))) %>% 
                          dplyr::mutate(events_per_hhid_stove = n()) %>%
                          dplyr::mutate(minutes_per_hhid_stove = sum(duration_minutes)) %>%
                          dplyr::ungroup() %>%
                          dplyr::mutate(qc = if_else(days_logged_by_hhid_stove < total_days_logged_minimum,"bad",qc)) %>% # Only keep data from house-stove combos with more than this number of days of data.
                          dplyr::arrange(HHID) %>%
                          dplyr::filter(grepl("ok",qc)) %>%#Keep only data marked 'ok'
                          dplyr::filter(!is.na(HHID)) %>%
                          dplyr::filter(!is.na(datetime_placed))  %>%
                          droplevels() #Getting rid of extra levels whos data was filtered out, so unique doesn't freak out.


ok_cooking_events_padded <- data.frame(stringsAsFactors = FALSE)
#Replace NA with 0.
#dplyr_if_else   <- function(x) { mutate_all(x, funs(ifelse(is.na(.), 0, .))) }
uniquers <- unique(ok_cooking_events$fullsumsarizer_filename)
for (i in 1:length(unique(ok_cooking_events$filename))) {

        temp <- dplyr::filter(ok_cooking_events,uniquers[i]==fullsumsarizer_filename) %>%
                dplyr::arrange(start_time)
        
    if (dim(temp)[1]>0) {    
        # generate a time sequence with 1 day intervals to fill in
        # missing dates
        all.dates <- data.frame(dates = seq(temp$datetime_placed[1], temp$datetime_removal[1], by="day"),stringsAsFactors = FALSE) %>%
                  dplyr::mutate(day_month_year = as.Date(dates)) %>%
                  dplyr::filter(!day_month_year %in% temp$day_month_year) #Get rid of extra days
        
        # Convert all dates to a data frame. Note that we're putting
        # the new dates into a column called "start_time" just like the
        # original column. This will allow us to merge the data.
        all.dates.frame <- data.frame(list(start_time=all.dates$dates),list(end_time=all.dates$dates), 
                                      list(day_month_year = as.Date(all.dates$dates)),
                                      list(week_year = format(all.dates$dates,"%V-%y")),
                                      list(day_of_week = weekdays(all.dates$dates)),stringsAsFactors = FALSE) %>%
                  dplyr::mutate(month_year = format(start_time,"%b-%y")) %>%
                  dplyr::mutate(month_year = factor(month_year, unique(month_year), ordered=TRUE))
                         

        # Merge the two datasets: the full dates and original data
        merged.data <- merge(all.dates.frame, temp, all=T) %>%
            tidyr::fill(filename,fullsumsarizer_filename,HHID,stove,logger_id,group,
                        stove_descriptions,logging_duration_days,datetime_placed,
                        datetime_removal,units,comments,
                        start_hour,events_per_hhid_stove,minutes_per_hhid_stove,
                        .direction = c("up")) %>%
            tidyr::fill(filename,fullsumsarizer_filename,HHID,stove,logger_id,group,
                        stove_descriptions,logging_duration_days,datetime_placed,
                        datetime_removal,units,comments,
                        start_hour,events_per_hhid_stove,minutes_per_hhid_stove,
                        .direction = c("down"))  %>%
            dplyr::mutate(use_flag = replace(use_flag, is.na(use_flag), FALSE)) 
        
        
        merged.data %>% mutate_if(is.factor, as.character) -> merged.data

        merged.data$use_flag[is.na(merged.data$use_flag)] <- FALSE
        merged.data[is.na(merged.data)] <- 0
        # The above merge set the new observations to NA.
        # To replace those with a 0, we must first find all the rows
        # and then assign 0 to them.
        #merged.data <-dplyr_if_else(merged.data)


        ok_cooking_events_padded <- rbind(ok_cooking_events_padded,merged.data)
    }
}
    #time format was messed up somehow in making the padded set, correct it here.
    ok_cooking_events_padded <- dplyr::mutate(ok_cooking_events_padded,datetime_removal = as.POSIXct(datetime_removal,origin = "1970-1-1", tz = "GMT")) %>%
      dplyr::mutate(qc = "ok") #New qc values default to zero during padding, but they are all already qc-filtered, so reset to TRUE



```



# Summarize data

```{r summarize_data}


ok_datasummary <- summarise_all(ok_cooking_events,funs(n_distinct(.)))

#Summarize use for each household and stove.
stats_grouped_by_hhid_stove <- dplyr::group_by(ok_cooking_events_padded,stove_descriptions,HHID,group) %>%
    # calculate events per day for each household and stove.  Can then take summary stats of those.
    dplyr::summarise(events_per_day=sum(use_flag,na.rm=TRUE)/length(unique(day_month_year)),
                     minutes_per_day_stove_hhid = sum(duration_minutes,na.rm=TRUE)/length(unique(day_month_year)),
                     minutes_per_event_stove_hhid = sum(duration_minutes,na.rm=TRUE)/sum(use_flag,na.rm=TRUE),
                     days_logged_per_group_by_stove=sum(length(unique(day_month_year)), na.rm = TRUE),
                     events = n()) 
kable(stats_grouped_by_hhid_stove, digits=2)


#Calculate summary for the overall groups and stoves from the household-wise results above.  Should this be weighted in case one hh has way more data?
stats_by_stove_group <- dplyr::group_by(stats_grouped_by_hhid_stove,stove_descriptions,group) %>%
    dplyr::summarise(mean_events_per_day=mean(events_per_day, na.rm = TRUE),
                       median_events_per_day=median(events_per_day, na.rm = TRUE),
                       sd_events_per_day=sd(events_per_day, na.rm = TRUE),
                       mean_minutes_per_day_stove_hhid=mean(minutes_per_day_stove_hhid, na.rm = TRUE),
                       median_minutes_per_day_stove_hhid=median(minutes_per_day_stove_hhid, na.rm = TRUE),
                       sd_minutes_per_day_stove_hhid=sd(minutes_per_day_stove_hhid, na.rm = TRUE),
                       mean_days_logged=mean(days_logged_per_group_by_stove, na.rm = TRUE),
                       house_stoves=n()) %>%
    dplyr::filter(!is.na(stove_descriptions)) 
kable(stats_by_stove_group, digits=2)


#Calculate summary for the overall stoves from the household-wise results above.  Should this be weighted in case one hh has way more data?
#Same as above but without the group, just by stove.
stats_by_stove <- dplyr::group_by(stats_grouped_by_hhid_stove,stove_descriptions) %>%
    dplyr::summarise(mean_events_per_day=mean(events_per_day, na.rm = TRUE),
                       median_events_per_day=median(events_per_day, na.rm = TRUE),
                       sd_events_per_day=sd(events_per_day, na.rm = TRUE),
                       mean_minutes_per_day_stove_hhid=mean(minutes_per_day_stove_hhid, na.rm = TRUE),
                       median_minutes_per_day_stove_hhid=median(minutes_per_day_stove_hhid, na.rm = TRUE),
                       sd_minutes_per_day_stove_hhid=sd(minutes_per_day_stove_hhid, na.rm = TRUE)) %>%
    dplyr::filter(!is.na(stove_descriptions)) 

kable(stats_by_stove, digits=2)


#Summarize use for each household and stove, by month
stats_grouped_by_hhid_stove_month <- dplyr::group_by(ok_cooking_events_padded,stove_descriptions,HHID,group,month_year) %>%
    # calculate events per day for each household and stove.  Can then take summary stats of those.
    dplyr::summarise(events_per_day=sum(use_flag,na.rm=TRUE)/length(unique(day_month_year)),
                     minutes_per_day_stove_hhid = sum(duration_minutes,na.rm=TRUE)/length(unique(day_month_year)),
                     minutes_per_event_stove_hhid = sum(duration_minutes,na.rm=TRUE)/sum(use_flag,na.rm=TRUE),
                     days_logged_per_group_by_stove=sum(length(unique(day_month_year)), na.rm = TRUE))
kable(stats_grouped_by_hhid_stove_month, digits=2)


#Calculate summary for the overall groups and stoves from the household-wise results above. 
stats_by_stove_group_month <- dplyr::group_by(stats_grouped_by_hhid_stove_month,stove_descriptions,group,month_year) %>%
    dplyr::summarise(mean_events_per_day=mean(events_per_day, na.rm = TRUE),
                       median_events_per_day=median(events_per_day, na.rm = TRUE),
                       sd_events_per_day=sd(events_per_day, na.rm = TRUE),
                       mean_minutes_per_day_stove_hhid=mean(minutes_per_day_stove_hhid, na.rm = TRUE),
                       median_minutes_per_day_stove_hhid=median(minutes_per_day_stove_hhid, na.rm = TRUE),
                       sd_minutes_per_day_stove_hhid=sd(minutes_per_day_stove_hhid, na.rm = TRUE),
                       households=n(),
                       mean_days_logged=mean(days_logged_per_group_by_stove, na.rm = TRUE))
kable(stats_by_stove_group_month, digits=2)


```


# Plot all data time series
* Plot usage rates (uses/day) by stove type, and region
```{r plot_all_timeseries, fig.width=10, fig.height=60}

#Plot time series for each household, colored by filename.  For data completion purposes.
field_timeseries_plot(sumsarized_filtered, "stove_temp", "datetime", "HHID", "stove_descriptions","qc") 

```

# Plot data time series, removing flagged files and households
* Plot usage rates (uses/day) by stove type, and region
```{r plot_ok_timeseries, fig.width=10, fig.height=60}
#Plot time series for each household, colored by filename.  Filtered out bad qc data
field_timeseries_plot(dplyr::filter(sumsarized_filtered,grepl("ok",qc)),"stove_temp", "datetime", "HHID", "stove_descriptions","qc") 

```

# List of files without associated HHID and metadata from the tracking sheet.
```{r list_orphaned_files, fig.width=10, fig.height=50}
files_without_hhid <- unique(dplyr::filter(sumsarized_filtered,is.na(HHID)) %>% dplyr::select(fullsumsarizer_filename))   #Data is removed in the next step if there is no HHID
files_without_hhid

files_without_metadata <- unique(dplyr::filter(sumsarized_filtered,is.na(stove_descriptions)) %>% dplyr::select(fullsumsarizer_filename))   #Note the name of files for which there is there is no matching filename in the tracking sheet, ergo no metadata from the tracking sheet. Not currently being filtered out, but this should be attended to by fixing any data files/tracking entries that appear in this variable.  Make sure that the actual file name matches what is listed in the tracking sheet.  Files names should also be unique.
print(files_without_metadata)
```


# Boxplot usage results

```{r boxplots_usage, fig.width=7, fig.height=7}


give.n <- function(x){
   return(c(y = 0, label = length(x)))
}

#To make box and whiskers quantiles rather than IQRs.
f <- function(x) {
  r <- quantile(x, probs = c(0.05, 0.25, 0.5, 0.75, 0.95))
  names(r) <- c("ymin", "lower", "middle", "upper", "ymax")
  r
}


#Plot average uses per day, by group, using the padded data set. (one data point per household-stove combo)
g1<- dplyr::group_by(ok_cooking_events_padded,stove_descriptions,HHID,group) %>%
  dplyr::summarise(avg_events_per_day=sum(use_flag,na.rm=TRUE)/length(unique(day_month_year))) %>%
  ggplot(aes(x=stove_descriptions, y = avg_events_per_day)) +
  stat_summary(fun.data = f, geom="boxplot") +  
    geom_jitter(height = 0,width = 0.2,alpha = 0.25) +
  #facet_grid(stove_use_category~group,scales = "free", space = "free") + 
  stat_summary(fun.data = give.n, geom = "text") + 
  facet_grid(~group,scales = "free", space = "free") + 
  labs(y="Average uses per day",x="") + 
  ggtitle("Average uses per day for each household-stove combination") + 
  theme(axis.text.x = element_text(angle = 30, hjust = 1)) + 
  theme(legend.title = element_blank())
g1
#ggsave(filename="../figures/AverageUsesPerDay_byGrouppadded.png", plot=g1,width = 6, height = 4)


#Plot uses per day, by group, using the padded data set. (one data point per day-household-stove combo)
g11<- dplyr::group_by(ok_cooking_events_padded, stove_descriptions, day_month_year, HHID, group) %>%
  summarise(events_per_day=sum(use_flag,na.rm=TRUE)) %>%
  ggplot(aes(events_per_day)) +
  geom_histogram() +
  #stat_summary(fun.data = f, geom="boxplot") +  
  #geom_jitter(height = 0,width = 0.2,alpha = 0.25) +
  #facet_grid(stove_use_category~group,scales = "free", space = "free") + 
  #stat_summary(fun.data = give.n, geom = "text") + 
  facet_grid(group~stove_descriptions,scales = "free", space = "free") + 
  labs(y="days",x="Number of uses by day") + 
  theme(axis.text.x = element_text(angle = 30, hjust = 1)) + 
  theme(legend.title = element_blank())
g11
##ggsave(filename="../figures/UsesPerDayDistribution_byGrouppadded.png", plot=g11,width = 6, height = 4)


#Plot average uses per day for all groups together
g2 <- dplyr::group_by(ok_cooking_events_padded,stove_descriptions,HHID) %>%
  dplyr::summarise(avg_events_per_day=sum(use_flag,na.rm=TRUE)/length(unique(day_month_year))) %>%
  ggplot(aes(x=stove_descriptions, y = avg_events_per_day)) +
  stat_summary(fun.data = f, geom="boxplot") +    geom_jitter(height = 0,width = 0.2,alpha = 0.25) + 
  labs(y="Average uses/day",x="") + 
  stat_summary(fun.data = give.n, geom = "text") + 
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) 
g2
#ggsave(filename="../figures/AverageUsesPerDay_All.png", plot=g2,width = 6, height = 4)

#Plot average minutes used per day for each stove-household combination
g3<-  dplyr::group_by(ok_cooking_events_padded,stove_descriptions,HHID,group) %>%
  dplyr::summarise(avg_events_per_day=sum(duration_minutes,na.rm=TRUE)/length(unique(day_month_year))) %>%
  ggplot(aes(x=stove_descriptions, y = avg_events_per_day)) +
  stat_summary(fun.data = f, geom="boxplot") +   geom_jitter(height = 0,width = 0.2,alpha = 0.25) +
  facet_grid(~group,scales = "free", space = "free") + 
  labs(y="Average minutes used per day",x="") + 
  stat_summary(fun.data = give.n, geom = "text") + 
  theme(axis.text.x = element_text(angle = 60, hjust = 1))
g3
#ggsave(filename="../figures/AverageTimeUsedPerDay_All.png", plot=g3,width = 6, height = 4)

#Average event duration
g4<-  dplyr::group_by(ok_cooking_events_padded,stove_descriptions,HHID,group) %>%
  dplyr::summarise(minutes_per_day_stove_hhid=sum(duration_minutes,na.rm=TRUE)/sum(use_flag,na.rm=TRUE)) %>%  
  ggplot(aes(x=stove_descriptions, y = minutes_per_day_stove_hhid)) + 
  stat_summary(fun.data = f, geom="boxplot") + 
  geom_jitter(height = 0,width = 0.2,alpha = 0.25) +
  stat_summary(fun.data = give.n, geom = "text") + 
  labs(y="Average event duration by household (minutes)",x="") + 
  theme(axis.text.x = element_text(angle = 60, hjust = 1))
g4
#ggsave(filename="../figures/AverageTimeUsedPerEvent_byHousehold.png", plot=g4,width = 6, height = 4)


```

# Time trend plots

```{r plot_time_trends, fig.width=10, fig.height=7}


#Plot event duration per day by day of week.  Do not use the padded data set here, since there are non-events included in it, and they have an associated time.
plotDayofWeekFunc <- function(x,y, na.rm = TRUE, ...) {
    g5 <- ggplot(dplyr::filter(x,grepl(y,group,ignore.case=TRUE)), aes(x=day_of_week, y = duration_minutes)) + 
      stat_summary(fun.data = f, geom="boxplot") +    geom_jitter(height = 0,width = 0.2,alpha = 0.25) +
      facet_wrap(stove_descriptions~group,scales = "free") + 
      labs(y="Cooking event duration (minutes)",x="") + 
      theme(axis.text.x = element_text(angle = 60, hjust = 1))
    g5
    #ggsave(g5,filename=paste("../figures/DurationbyDayofWeek",y,".png",sep=""))
}

for (i in 1:dim(stove_group_codes)[1]){
  g5 <- plotDayofWeekFunc(ok_cooking_events,as.character(stove_group_codes$group[i]))
}

#Monthly usage summary
plotMonthlyFunc <- function(x,y, na.rm = TRUE, ...) {
    monthlyboxplots <-    ggplot(x,aes(x=month_year, y = mean_events_per_day,color = stove_descriptions)) +
        stat_summary(fun.data = f, geom="boxplot",position = position_dodge(width=0.75)) +
        geom_jitter(alpha = 0.25,position = position_dodge(width=0.75)) +
        labs(y="Mean events/day",x="") + 
        ggtitle(paste(y)) + 
        theme(axis.text.x = element_text(angle = 60, hjust = 1),legend.title = element_blank())
    monthlyboxplots
    #ggsave(monthlyboxplots,filename=paste("../figures/monthlyboxplots",y,".png",sep=""),width = 7, height = 2)
}
 
for (i in 1:dim(stove_group_codes)[1]){
  plotMonthlyFunc(stats_grouped_by_hhid_stove_month,as.character(stove_group_codes$group[i]))
}


#Plot %stove-days, where stove-day is defined as whether the stove is used on the given day or not.
plotStoveDaysFunc <- function(ok_cooking_events_padded, na.rm = TRUE, ...) {
  #Calculate percent stove-days.
  g7 <-dplyr::group_by(ok_cooking_events_padded,day_month_year,stove_descriptions,group) %>%
        dplyr::distinct(HHID,stove_descriptions,day_month_year, .keep_all = TRUE) %>% #No duplicate events from the same hh contributing to stove-days.
        dplyr::mutate(days_used_day_stove_grouped = 100*sum(use_flag, na.rm=TRUE)/(sum(use_flag, na.rm=TRUE)+sum(!use_flag, na.rm=TRUE))) %>%
        ggplot(aes(x=day_month_year, y = days_used_day_stove_grouped,color = stove_descriptions)) + 
      geom_point() + geom_smooth() + #method='lm',formula=y~x) +
      facet_grid(~group,scales = "free", space = "free") + 
      labs(y="% stove days",x="") + 
      theme(legend.title = element_blank()) + 
      theme(axis.text.x = element_text(angle = 60, hjust = 1))
    g7
          #ggsave(g7,filename=paste("../figures/PercentStoveDaysByGroup.png",sep=""))
}
g7 <- plotStoveDaysFunc(ok_cooking_events_padded)


#Time of day trend.  Based on start time, change it to middle of event?  Remove groups with less than 5 hh's?
#Use unpadded set to get time of day of event starts.
plotDensFunc <- function(x,y, na.rm = TRUE, ...) {
    g8 <- ggplot(x, aes(start_hour, fill = stove_descriptions, 
                          colour = stove_descriptions)) + geom_density(alpha = 0.1) +
          labs(y="Time of day use density",x="Hour of day") +
          ggtitle(paste(y,"stove use density")) + 
          theme(plot.title = element_text(hjust = 0.5)) +
          ylim(0, 0.15)
    g8
    #ggsave(g8,filename=paste("../figures/Density",y,".png",sep=""))
}

for (i in 1:dim(stove_group_codes)[1]){
  ok_cooking_events_temp <- dplyr::filter(ok_cooking_events,grepl(as.character(stove_group_codes$group[i]),filename))
  g8 <- plotDensFunc(ok_cooking_events_temp,as.character(stove_group_codes$group[i]))
}


#Usage time series by minutes used each day, by group and stove type  
 g12<- dplyr::group_by(ok_cooking_events_padded,stove_descriptions,day_month_year,group) %>%
  dplyr::summarise(mean_daily_use_tseries=mean(duration_minutes,na.rm=TRUE)) %>%
  ggplot(aes(day_month_year, y=mean_daily_use_tseries,color = stove_descriptions)) + #,label=sprintf("%0.2f", round(usefraction, digits = 2)))) +
  geom_smooth()+
  geom_point(alpha = 0.25)+
  facet_grid(~group~stove_descriptions,scales = "free", space = "free") + 
  ggtitle("Daily average used by group and stove") + 
  labs(y="Minutes per day",x="") +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + 
  ylim(0,200)
 g12
#ggsave(filename="../figures/Overall_usage_fraction.png", plot=g12,width = 6, height = 4)

```

# Usage fraction result plots

```{r plot_usage_fractions, fig.width=7, fig.height=7}

#Daily usage fraction by group. Includes all hh.  We could filter out houses with fewer than x sums to reduce any potential bias from a certain sum type burning up or something.
g10<- dplyr::select(ok_cooking_events_padded,stove_descriptions,start_time,duration_minutes) %>%
  thicken('day', col = 'day') %>% 
  group_by(stove_descriptions, day) %>%
  summarise(avg=mean(duration_minutes,na.rm=TRUE)) %>%
  ggplot(aes(day, avg)) +
  geom_bar(aes(fill = stove_descriptions), 
           col = "black",
           stat = 'identity', 
           position = "fill",size=0) +
  ggtitle("Usage fraction") + 
  labs(y="Usage fraction",x="")  +
  theme(legend.title = element_blank())
g10

#Overall usage fraction by group.  
 g11<- dplyr::select(ok_cooking_events_padded,stove_descriptions,day_month_year,duration_minutes) %>%
  group_by(stove_descriptions) %>%
  summarise(avg=mean(duration_minutes,na.rm=TRUE)) %>%
  dplyr::mutate(usefraction = avg/sum(avg)) %>%
  ggplot(aes(x=1, usefraction,label=sprintf("%0.2f", round(usefraction, digits = 2)))) +
  geom_bar(aes(fill = stove_descriptions), 
           col = "black",
           stat = 'identity', 
           position = "fill",size=0) +
  geom_text(size = 5, position = position_stack(vjust = 0.5))+
  ggtitle("Overall usage fraction") + 
  labs(y="Usage fraction",x="")  +
  theme(legend.title = element_blank())
g11
#ggsave(filename="../figures/Overall_usage_fraction.png", plot=g12,width = 6, height = 4)


#Overall usage fraction by group.  
 g111<- dplyr::select(ok_cooking_events_padded,stove_descriptions,day_month_year,duration_minutes,group) %>%
  group_by(stove_descriptions,group) %>%
  summarise(avg=mean(duration_minutes,na.rm=TRUE)) %>%
  dplyr::mutate(usefraction = avg/sum(avg)) %>%
  ggplot(aes(x=1, usefraction,label=sprintf("%0.2f", round(usefraction, digits = 2)))) +
  geom_bar(aes(fill = stove_descriptions), 
           col = "black",
           stat = 'identity', 
           position = "fill",size=0) +
  geom_text(size = 5, position = position_stack(vjust = 0.5))+
  ggtitle("Overall usage fraction") + 
  facet_grid(~group,scales = "free", space = "free") + 
  labs(y="Usage fraction",x="")  +
  theme(legend.title = element_blank())
g111
#ggsave(filename="../figures/Overall_usage_fraction_bygroup.png", plot=g111,width = 6, height = 4)
```

# Plot deployment details 

```{r plot_monitoring_diagnostics, fig.width=7, fig.height=}

 #Barplot by day of fraction of stoves monitored... not too interesting but neat coding to look at.
g9<- dplyr::select(ok_cooking_events_padded,stove_descriptions,start_time,duration_minutes) %>%
  thicken('day', col = 'day') %>% #Map data to a higher level variable.  Compresses two data points from one day into one day
  count(stove_descriptions, day) %>%
  ggplot(aes(day, n)) +
  ggtitle("Daily stove monitored by fraction") + 
  labs(y="Monitored fraction",x="")  +
  geom_bar(aes(fill = stove_descriptions), 
           col = "black",
           stat = 'identity', 
           position = "fill", size=0)
g9

#Plot days sampled per file
g6<-  dplyr::distinct(ok_cooking_events_padded,fullsumsarizer_filename,logging_duration_days,stove_descriptions) %>% 
  ggplot(aes(x=stove_descriptions, y = logging_duration_days)) + 
  stat_summary(fun.data = f, geom="boxplot") + geom_jitter(width = 0.05,alpha = 0.25) + 
  #geom_text_repel( aes(label= ifelse(logging_duration_days > 30,as.character(fullsumsarizer_filename),''))) + 
  #facet_grid(~group,scales = "free", space = "free") + 
  labs(y="Days logged per file",x="") + 
  theme(axis.text.x = element_text(angle = 60, hjust = 1))
g6
#ggsave(filename="../figures/DaysSampled_byGroup.pdf", plot=g6)
```



# Summary

Temperature was measured for `r length(unique(ok_cooking_events$HHID))` houses between `r min(ok_cooking_events$start_time, na.rm = TRUE)` and `r max(ok_cooking_events$end_time, na.rm = TRUE)`. There are no cooking events for homes: `r setdiff(as.character(metadata$HHID), as.character(ok_cooking_events$HHID))`.

Temperature data is expected to be missing for: no tests.

# Save files

* save data

```{r save_data}
  saveRDS(ok_cooking_events, file = "../r_files/ok_cooking_events.RDS")
```


